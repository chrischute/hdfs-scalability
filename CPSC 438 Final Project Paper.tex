% CPSC 438 Final Project Paper
% Christopher Chute, David Brandfonbrener, Leo Shimonaka, Matthew Vasseur

% Author and Title Information
\newcommand*{\thetitle}{Scalability Limits of HDFS}
\newcommand*{\theauthor}{Christopher Chute, David Brandfonbrener, Leo Shimonaka, Matthew Vasseur}
\newcommand*{\duedate}{May 2, 2016}

% Document Settings
\documentclass[11pt, a4paper]{article}
\author{\theauthor}
\title{\thetitle}
\date{\duedate}
\usepackage[top=.75in, left=0.5in, right=0.5in, bottom=1.5in]{geometry}
\usepackage{amsmath, amsthm, amssymb}
\newtheorem{lem}{Lemma}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage{titling}

% Multicolumn Formatting
\usepackage{multicol}
\setlength\columnsep{18pt}    % spacing between columns

\setlength{\droptitle}{-9em}
\posttitle{\par\end{center}\vspace{-.35em}}

% Header Formatting
\usepackage{fancyhdr}
\setlength{\headheight}{48pt}
\pagestyle{fancyplain}
%\lhead{\thetitle}
%\rhead{\theauthor\\\duedate}
%\rfoot{}
\cfoot{\thepage}

% ************************* End of Preamble ***********************
\begin{document}
\maketitle
\thispagestyle{empty}

\begin{abstract}
% TODO: Complete after paper done.
\end{abstract}
\begin{multicols*}{2}


\section{Introduction}

% Introduce HDFS and the notion of Distributed FileSystems, their prevalence, etc
% I.e. Some B.S. on what's out there to set the background

Distributed file systems (DFS), also known as network file systems, stores remote files in a format accessible with the same semantics as a local filesystem. It offers a transparent interface for handling files and directories, abstracting features like access control, recovery and concurrency. DFS offers a scalable mechanism for creating logical filesystems shared by many clients.

The DFS architecture has many advantages; it offers a unified hierarchical namespace for shared files and folders, location transparency in physical data placement, load balancing across nodes, storage scalability via expanding clusters, increased availability through distributed data, and fault tolerance by replication across racks.

Many DFS implementations exist today. Each system differs in their performance, data mutability, handling of concurrent writes, failure handling, and storage policies. These include POSIX-compliant systems like Lustre, search-engine enhanced Google File System (GFS), and non-proprietary open-source DFS like Hadoop Distributed File System.

This paper examines Hadoop Distributed File System (HDFS) implemented in Java. HDFS is a distributed filesystem focused on scability and portability. HDFS is not fully POSIX-compliant since HDFS is designed for handling large files terabytes in size. It trades the functionality such as mutable files for increased data throughput and non-POSIX operations such as appending.

    

% -------------------------------------------------

\section{HDFS Architecture}

%TODO figures???

% Describe the 5 Hadoop design principles (Perhaps move to Introduction)

The architecture of HDFS is designed with the following goals in mind:
\begin{enumerate}[noitemsep, label=\arabic*.]
	\item\textit{Support regular hardware failure.} This is accomplished primarily through replication of data across multiple DataNodes.
	\item\textit{Allow streaming data access.} Partitioning should be transparent so that files appear to be stored sequentially. Reads are distributed so that streaming access does not block other transactions.
	\item\textit{Support very large data sets.} To provide scalability, more DataNodes can be added to support large data sets. This assumes that the NameNode memory limit will not become a bottleneck before the disk space of the DataNodes.
	\item\textit{Provide simple concurrency control.} The write-once, read-many model mainly accomplishes this. Only appends are allowed, and only one appender/creator per file is allowed at once. This allows HDFS to provide highly available and concurrent reads.
	\item\textit{Run everywhere (portability).} HDFS runs on top of the Java Virtual Machine, which provides nearly universal portability.
\end{enumerate}


% Describe typical HDFS cluster: Single NameNode, Multiple DataNodes
% The goal for decoupling metadata from actual data is scalability

With these principles in mind, the HDFS architecture is designed as follows. An HDFS cluster consists of a single NameNode and many DataNodes. Briefly, the NameNode holds the metadata for each file in the file system. Metadata for a directory consists of an inode, and metadata for a file consists of an (inode, mapping) pair. The inode is a UNIX-style inode and thus contains properties such as permissions and ownership. Moreover, an HDFS inode stores HDFS-specific attributes such as quotas and replication factors.

% TODO: Say which section the linear block result is presented in.
Each file is partitioned into one or more blocks, and each block is replicated across multiple DataNodes. The mapping for a file maps each block of the file to a list of DataNodes where the block is replicated. As presented below, the metadata associated with each file scales linearly with the number of blocks in each file. That is, there is a fixed size mapping for each block in the file. The NameNode holds all of this metadata in memory to facilitate fast metadata operations.

% Benefits of Single Namenode Architecture:
% 1) Given an operation, all namespace operations run on single Namenode while expensive data transfers are distributed across Datanodes of cluster
% 2) Simple durability scheme via replication of the same data block across Datanodes.
% 3) Same replication technique over independent DataNodes also enable a highly available system exploiting principles of MapReduce.
A single NameNode is used primarily to simplify the architecture and metadata management of HDFS. Since all metadata is stored on the NameNode First, metadata operations are relatively inexpensive compared to the I/O-intensive operations performed on DataNodes. Thus it is likely that  NameNode and never block each other. If each node had to handle its own metadata operations, the metadata operation to indicate which node to read from could be slowed by threads performing large data operations at the node being queried. Second, a single NameNode architecture allows for a simple durability scheme via replication of each block of data across many DataNodes organized by the central NameNode. If the metadata was not centralized, this replication operation and updating all data mappings would become complicated, and potentially overuse the network connecting the nodes.

Now we will examine certain aspects of the specific HDFS architecture in more detail. 

\subsection{NameNode and DataNodes}

% The entire metadata namespace is kept in memory. 
% In-memory namespace keeps operations from external clients (get_block_locations, create_block) and internal communication (heartbeats, block reports) fast
% File that maps UNIX-style inodes to data block locations in Datanodes
% Write-Ahead log in stable storage

All metadata is kept in memory on the NameNode. As explained above, the metadata holds a UNIX-style inode and location on DataNodes for each file. These locations are represented as block locations in virtual memory of each of the DataNodes. Meaning that upon creation of a file, the NameNode will allocate a block for the file in virtual memory on however many DataNodes the file will be replicated across (the default is 3 replicas). Then, the NameNode is responsible for pointing the clients to the appropriate blocks to write or read files, and also maintaining the namespace. The NameNode keeps the maps to data blocks up-to-date by communicating with the DataNodes via ``heartbeats" (i.e., TCP packets). The DataNodes are responsible for sending periodic heartbeats that contain block reports to indicate that they are functioning correctly and can be reached along the network. The block report gives the block locations of all files in the virtual memory of that DataNode. The NameNode uses this information to ensure each block is replicated enough times and keep the locations of the blocks up to date. 

Clearly durability of the NameNode is very important to the system. There are various schemes to ensure a durable NameNode. A write ahead log is maintained on stable storage and this can be used to build a BackupNode or CheckpointNode that essentially maintains a copy of the NameNode. 


%\subsection{DataNode}

% Stores file's data blocks
% Periodic heartbeat to notify Namenode of its health (i.e. if it's running)
% Periodic block reports to notify Namenode of a stored block's status (e.g. block write recieved)

%TODO cut this section/ merge with above?



\subsection{Reads and Writes in HDFS}
% Perhaps add section for how Read / Write operations are processed in HDFS? Might help describe the type of operations Namenode has to process.
% E.g. Client Request Workflow for reading data

% -------------------------------------------------

Files in HDFS are append-only and can only be written to by a single writer at a time so that once data is written it cannot be overwritten. This simplifies concurrency control and allows the system to guarantee that writes can be read as soon as a file is closed. On a write, a client must first query the NameNode to find which DataNodes and blocks to write. Then, the client will ensure a replicated write by sending data in TCP-style packets through a pipeline including all DataNodes over which the file will be replicated. This guarantees that the writes will be consistent across multiple DataNodes. On a read, a client must first query the NameNode to find which DataNodes contain replicas of the desired file and which blocks hold the file. Then, the client reads from the closest available DataNodes directly. 


\section{Limitations of Single Namenode Architecture}

Despite the benefits of the single NameNode architecture listed above, this architectural decision causes some potentially major scalability problems. These problems will be especially pronounced when HDFS is used to store many small files as opposed to few large files (although many large files will also prove difficult HDFS). 

\subsection{Physical Memory Size}

% Re-iterate in-memory metadata namespace
% Total File Capacity of Cluster is Limited by Name Node's RAM due to requirement that all metadata objects (file inodes and block) in memory at all times

% SVENCHO PAPER EXAMPLE: Assuming on average, a file consists of \lambda = 1.5 blocks, then each file uses 1 file and ~2 data block objects, hence uses ~600 bytes. Thus to keep 100 million files, name-node must have at least 60GM of RAM


The obvious problem with forcing all metadata to be in memory on the NameNode is that the NameNode will run out of available memory. %todo cite Shvanko

Let $ M $ be the size of memory on the NameNode in bytes, $ \lambda $ be the number of blocks allocated to each file, and $ F $ be the number of files on the system. Also, let $ c $ be the number of bytes to represent a single block mapping to all replicas of a block, and $ i $ the number of bytes to represent and inode. If we want to fit all metadata into memory, it is clear that we must have:
\begin{align*}
	M > \lambda c F + i F
\end{align*}
	This limit is much easier to reach with small files since each file is allocated its own virtual block, no matter how small the file is, i.e. we have that $ \lambda \geq 1$. This means that creating many small files should take up $ c + i$ bytes for each additional file, while growing a file by a similar amount will use at most $ c $ bytes. Thus, the memory limit is much more of a problem for many small files. 


\subsection{Namenode CPU Bottleneck}

The less obvious scalability problem with the HDFS architecture is the potential for a CPU bottleneck at the NameNode. The system is built on the assumption that the NameNode operations are small enough and there will be few enough large files that they will be insignificant. However, with many small files, these assumptions are violated and the NameNode CPU could become a bottleneck. 

% Describe the INTERNAL-LOAD communicating with Datanodes
% # of Heartbeats that Namenode recieves is directly proportional to # of Datanodes in cluster
% # of Block reports is also directly proportional to size of data blocks (i.e. ratio of blocks mapped per file) and also # of replicated copies per block
% Physical data storage and data I/O performance increase proportionally to # of Datanodes in cluster and size of data blocks, but the overall performance of HDFS is also affected negatively by increased internal load.
% The more time spend on internal load, less time spend on processing external client's requests

The first potential source of CPU bottleneck is the internal-load of communication with the DataNodes. This internal-load can be created in two ways. First, internal-load can be created when the system holds a large amount of small files. The size of the block report sent by each DataNode is directly proportional to the number of blocks that the DataNode holds. Since each file is given its own virtual block, even if the file is much smaller than a block, the DataNode can potentially hold a great number of small files and have to send a large block report containing the metadata of each of these files. This could potentially overload the NameNode CPU. 
Second, the number of block reports that a NameNode receives is directly proportional to the number of DataNodes in the system. So, if the system has a large number of DataNodes, it is possible to generate a large amount of internal load by having many DataNodes.  


% Describe the external-load from clients
% All metadata operations run on Namenode - they cannot be processed anywhere else.
% On every file read, there is a get_block_location per block per file
% On every file write, there is a create_block per block per file
% Namenode can become a bottleneck with operations that involve many metadata operations, e.g. batch-write operations will include many create_block requests

% -------------------------------------------------

Another potential source of CPU bottleneck is external-load from clients querying the NameNode. As explained above, on both reads and writes a client must first contact the NameNode to find the appropriate block locations and then the DataNodes for the read or write. Afterwards the NameNode must perform a metadata operation to determine what to return to the client. Therefore, the NameNode can become a bottleneck if there are many clients trying to perform operations concurrently since they all must access the metadata via the singular NameNode. This should be an especially prevalent problem when there are many small files because it would be likely that the use pattern of a system with many small files would be for there to be many clients each trying to access their files concurrently. 

With these potential CPU bottlenecks in mind, we pose the question of whether it is possible to generate an appropriate load so as to reach these CPU bottlenecks before the NameNode runs out of memory. This will be addressed by our experimentation. 

\section{Performance Evaluation}

% Discuss goals of tests (i.e. "we want to test Memory and CPU and determine blah")

% Mention experimental setup (EC2, Java Test Skeleton, # nodes, etc)

% FOR EACH EXPERIMENT:
% Discuss designs of individual experiments and their results
% What were the experiments testing?
% What were the different parameters of the experiments?
% Possibilities of error?

\subsection{Experiment 1: Memory Experiments}

% TODO: See above

\subsection{Experiment 2: CPU Experiments}

% TODO: See above

\subsection{Analysis}

% Discuss raw results independently for each experiment:
% Are these results what was expected (and why)?
% If there was some error, discuss what may have caused it
% Graphs

% Evaluate experimental results holistically to pinpoint the problem with HDFS's Single Namenode architecture (I.e. weigh CPU and Memory problems against each other)

% -------------------------------------------------

\section{Improvements to Single NameNode Architecture}

% Use experimental data to determine what is actually detrimental to performance of HDFS, and suggest a way to improve it

% -------------------------------------------------

% Would it be helpful to remove the assumption that each file is assigned to its own block? Or alternatively, somehow batch block-reports for small files

% mention HDFS federation, CalvinFS
% 

\section{Conclusion}

% Wrap it up with stuff.



%TODO sources

\end{multicols*}
\end{document}


























 ``