% CPSC 438 Final Project Paper
% Christopher Chute, David Brandfonbrener, Leo Shimonaka, Matthew Vasseur

% Author and Title Information
\newcommand*{\thetitle}{Scalability Limits of HDFS}
\newcommand*{\theauthor}{Christopher Chute, David Brandfonbrener, Leo Shimonaka, Matthew Vasseur}
\newcommand*{\duedate}{May 2, 2016}

% Document Settings
\documentclass[11pt, a4paper]{article}
\author{\theauthor}
\title{\thetitle}
\date{\duedate}
\usepackage[top=.75in, left=0.5in, right=0.5in, bottom=1.5in]{geometry}
\usepackage{amsmath, amsthm, amssymb}
\newtheorem{lem}{Lemma}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage{titling}

% Multicolumn Formatting
\usepackage{multicol}
\setlength\columnsep{18pt}    % spacing between columns

\setlength{\droptitle}{-9em}
\posttitle{\par\end{center}\vspace{-.35em}}

% Header Formatting
\usepackage{fancyhdr}
\setlength{\headheight}{48pt}
\pagestyle{fancyplain}
%\lhead{\thetitle}
%\rhead{\theauthor\\\duedate}
%\rfoot{}
\cfoot{\thepage}

% ************************* End of Preamble ***********************
\begin{document}
\maketitle
\thispagestyle{empty}

\begin{abstract}
% TODO: Complete after paper done.
\end{abstract}
\begin{multicols*}{2}

% -------------------------------------------------

\section{Introduction}

% Introduce HDFS and the notion of Distributed FileSystems, their prevalence, etc
% I.e. Some B.S. on what's out there to set the background

% -------------------------------------------------

\section{HDFS Architecture}

%TODO figures???

% Describe the 5 Hadoop design principles (Perhaps move to Introduction)

Hadoop claims to be built on five core principles that govern the architectural decisions. These are (1) hardware failure is the norm. As a result, HDFS wants to be highly reliable in the face of hardware failure so the architecture incorporates replication of data across multiple nodes. Then HDFS wants to (2) allow streaming data access and (3) support very large data sets. With these goals in mind, HDFS wants to use a distributed system to accommodate large files and implement distributed reads to prevent streaming access from clogging the system. Next HDFA wants to (4) provide simple concurrency control with a write-once read-many model. This is less a feature and more a decision to weigh reads over writes by providing highly available and concurrent reads at the expense of allowing concurrent writes. And lastly, HDFS wants (5) portability, which makes sense as HDFS must work on different computer architectures 

% Describe typical HDFS cluster: Single Namenode, Multiple Datanodes
% The goal for decoupling metadata from actual data is scalability

To satisfy these principles, the HDFS architecture was designed as follows. An HDFS cluster will have one unique NameNode and many DataNodes. Briefly, the NameNode holds the metadata of each file in the system. This metadata consists of (inode, mapping) pairs where the inode is a UNIX-style inode representing the file and the mapping holds the information about where the file resides in disk on the DataNodes. The NameNode holds all of this metadata in memory to facilitate fast metadata operations. 

% Benefits of Single Namenode Architecture:
% 1) Given an operation, all namespace operations run on single Namenode while expensive data transfers are distributed across Datanodes of cluster
% 2) Simple durability scheme via replication of the same data block across Datanodes. 
%TODO What does 3 mean here?
% 3) Same replication technique over independent Datanodes also enable a highly available system exploiting principles of MapReduce.

This single NameNode architecture is beneficial in a few ways. First, this facilitates large reads since the reads are distributed among the DataNodes and the relatively inexpensive metadata operations all happen on the NameNode and never block each other. If each node had to handle its own metadata operations, the metadata operation to indicate which node to read from could be slowed by threads performing large data operations at the node being queried. Second, a single NameNode architecture allows for a simple durability scheme via replication of each block of data across many DataNodes and organized by the central NameNode. If the metadata was not centralized, this replication operation and updating all data mappings would become complicated, and potentially overuse the network connecting the nodes.

Now we will examine certain aspects of the specific HDFS architecture in more detail. 

\subsection{NameNode and DataNodes}

% The entire metadata namespace is kept in memory. 
% In-memory namespace keeps operations from external clients (get_block_locations, create_block) and internal communication (heartbeats, block reports) fast
% File that maps UNIX-style inodes to data block locations in Datanodes
% Write-Ahead log in stable storage

All metadata is kept in memory on the NameNode. As explained above, the metadata holds a UNIX-style inode and location on DataNodes for each file. These locations are represented as block locations in virtual memory of each of the DataNodes. So, upon creation of a file, the NameNode will allocate a block for the file in virtual memory on however many DataNodes the file will be replicated across (the default is 3 replicas). Then, the NameNode is responsible to pointing clients to the appropriate blocks to write or read files, and also to maintain the namespace. The NameNode keeps these maps to data blocks up to date by communicating with the DataNodes via heartbeats. The DataNodes are responsible for sending periodic heartbeats to indicate that they are functioning correctly that contain block reports. The block report gives the block locations of all files in the virtual memory of the DataNode. The NameNode uses this information to ensure each block is replicated enough times and to keep the locations of the blocks up to date. 

Clearly durability of the NameNode is very important to the system. There are various schemes to ensure a durable NameNode. A write ahead log is maintained on stable storage and this can be used to build a BackupNode or CheckpointNode that essentially maintain a copy of the NameNode. 


%\subsection{DataNode}

% Stores file's data blocks
% Periodic heartbeat to notify Namenode of its health (i.e. if it's running)
% Periodic block reports to notify Namenode of a stored block's status (e.g. block write recieved)

%TODO cut this section/ merge with above?



\subsection{Reads and Writes in HDFS}
% Perhaps add section for how Read / Write operations are processed in HDFS? Might help describe the type of operations Namenode has to process.
% E.g. Client Request Workflow for reading data

% -------------------------------------------------

Files in HDFS are append-only and can only be written to by a single writer at a time so that once data is written it cannot be overwritten. This simplifies concurrency control and allows the system to guarantee that writes can be read as soon as a file is closed. On a write, a client must first query the NameNode to find which DataNodes to write to and which blocks to write to on those DataNodes. Then, the client will ensure a replicated write by sending data in TCP-style packets through a pipeline including all DataNodes over which the file will be replicated. This guarantees that the writes will be consistent across multiple DataNodes. On a read, a client must first query the NameNode to find which DataNodes contain replicas of the desired file and which blocks hold the file. Then, the client reads from the closest available DataNode directly. 


\section{Limitations of Single Namenode Architecture}

Despite the benefits of the single NameNode architecture listed above, this architectural decision causes some potentially major scalability problems. 

\subsection{Physical Memory Size}

% Re-iterate in-memory metadata namespace
% Total File Capacity of Cluster is Limited by Name Node's RAM due to requirement that all metadata objects (file inodes and block) in memory at all times

% SVENCHO PAPER EXAMPLE: Assuming on average, a file consists of \lambda = 1.5 blocks, then each file uses 1 file and ~2 data block objects, hence uses ~600 bytes. Thus to keep 100 million files, name-node must have at least 60GM of RAM





\subsection{Namenode CPU Bottleneck}

% Describe the INTERNAL-LOAD communicating with Datanodes
% # of Heartbeats that Namenode recieves is directly proportional to # of Datanodes in cluster
% # of Block reports is also directly proportional to size of data blocks (i.e. ratio of blocks mapped per file) and also # of replicated copies per block
% Physical data storage and data I/O performance increase proportionally to # of Datanodes in cluster and size of data blocks, but the overall performance of HDFS is also affected negatively by increased internal load.
% The more time spend on internal load, less time spend on processing external client's requests


% Describe the external-load from clients
% All metadata operations run on Namenode - they cannot be processed anywhere else.
% On every file read, there is a get_block_location per block per file
% On every file write, there is a create_block per block per file
% Namenode can become a bottleneck with operations that involve many metadata operations, e.g. batch-write operations will include many create_block requests

% -------------------------------------------------

\section{Performance Evaluation}

% Discuss goals of tests (i.e. "we want to test Memory and CPU and determine blah")

% Mention experimental setup (EC2, Java Test Skeleton, # nodes, etc)

% FOR EACH EXPERIMENT:
% Discuss designs of individual experiments and their results
% What were the experiments testing?
% What were the different parameters of the experiments?
% Possibilities of error?

\subsection{Experiment 1: Memory Experiments}

% TODO: See above

\subsection{Experiment 2: CPU Experiments}

% TODO: See above

\subsection{Analysis}

% Discuss raw results independently for each experiment:
% Are these results what was expected (and why)?
% If there was some error, discuss what may have caused it
% Graphs

% Evaluate experimental results holistically to pinpoint the problem with HDFS's Single Namenode architecture (I.e. weigh CPU and Memory problems against each other)

% -------------------------------------------------

\section{Improvements to Single Namenode Architecture}

% Use experimental data to determine what is actually detrimental to performance of HDFS, and suggest a way to improve it

% -------------------------------------------------

\section{Conclusion}

% Wrap it up with stuff.



%TODO sources

\end{multicols*}
\end{document}


























 ``