% CPSC 438 Final Project Paper
% Christopher Chute, David Brandfonbrener, Leo Shimonaka, Matthew Vasseur

% Author and Title Information
\newcommand*{\thetitle}{Scalability Limits of HDFS}
\newcommand*{\theauthor}{Christopher Chute, David Brandfonbrener, Leo Shimonaka, Matthew Vasseur}
\newcommand*{\duedate}{May 2, 2016}

% Document Settings
\documentclass[11pt, a4paper]{article}
\author{\theauthor}
\title{\thetitle}
\date{\duedate}
\usepackage[top=.75in, left=0.5in, right=0.5in, bottom=1.5in]{geometry}
\usepackage{amsmath, amsthm, amssymb}
\newtheorem{lem}{Lemma}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage{titling}

% Multicolumn Formatting
\usepackage{multicol}
\setlength\columnsep{18pt}    % spacing between columns

\setlength{\droptitle}{-9em}
\posttitle{\par\end{center}\vspace{-.35em}}

% Header Formatting
\usepackage{fancyhdr}
\setlength{\headheight}{48pt}
\pagestyle{fancyplain}
%\lhead{\thetitle}
%\rhead{\theauthor\\\duedate}
%\rfoot{}
\cfoot{\thepage}

% ************************* End of Preamble ***********************
\begin{document}
\maketitle
\thispagestyle{empty}

\begin{abstract}
% TODO: Complete after paper done.
\end{abstract}
\begin{multicols*}{2}

% -------------------------------------------------

\section{Introduction}

% Introduce HDFS and the notion of Distributed FileSystems, their prevalence, etc
% I.e. Some B.S. on what's out there to set the background

% -------------------------------------------------

\section{HDFS Architecture}

%TODO figures???

% Describe the 5 Hadoop design principles (Perhaps move to Introduction)

Hadoop claims to be built on five core principles that govern the architectural decisions. These are (1) hardware failure is the norm. As a result, HDFS wants to be highly reliable in the face of hardware failure so the architecture incorporates replication of data across multiple nodes. Then HDFS wants to (2) allow streaming data access and (3) support very large data sets. With these goals in mind, HDFS wants to use a distributed system to accommodate large files and implement distributed reads to prevent streaming access from clogging the system. Next HDFS wants to (4) provide simple concurrency control with a write-once read-many model. This is less a feature and more a decision to weigh reads over writes by providing highly available and concurrent reads at the expense of allowing concurrent writes. And lastly, HDFS wants (5) portability, which makes sense as HDFS must work on different computer architectures 

% Describe typical HDFS cluster: Single Namenode, Multiple Datanodes
% The goal for decoupling metadata from actual data is scalability

To satisfy these principles, the HDFS architecture was designed as follows. An HDFS cluster will have one unique NameNode and many DataNodes. Briefly, the NameNode holds the metadata of each file in the system. This metadata consists of (inode, mapping) pairs where the inode is a UNIX-style inode representing the file and the mapping holds the information about where the file resides in disk on the DataNodes. Also included in the metadata are filesystem properties such as permissions and ownership per directory or file as well as HDFS attributes such as quotas and replication factors. The NameNode holds all of this metadata in memory to facilitate fast metadata operations. 

% Benefits of Single Namenode Architecture:
% 1) Given an operation, all namespace operations run on single Namenode while expensive data transfers are distributed across Datanodes of cluster
% 2) Simple durability scheme via replication of the same data block across Datanodes. 
%TODO What does 3 mean here?
% 3) Same replication technique over independent Datanodes also enable a highly available system exploiting principles of MapReduce.

This single NameNode architecture is beneficial in a few ways. First, this facilitates large reads since the reads are distributed among the DataNodes and the relatively inexpensive metadata operations all happen on the NameNode and never block each other. If each node had to handle its own metadata operations, the metadata operation to indicate which node to read from could be slowed by threads performing large data operations at the node being queried. Second, a single NameNode architecture allows for a simple durability scheme via replication of each block of data across many DataNodes organized by the central NameNode. If the metadata was not centralized, this replication operation and updating all data mappings would become complicated, and potentially overuse the network connecting the nodes.

Now we will examine certain aspects of the specific HDFS architecture in more detail. 

\subsection{NameNode and DataNodes}

% The entire metadata namespace is kept in memory. 
% In-memory namespace keeps operations from external clients (get_block_locations, create_block) and internal communication (heartbeats, block reports) fast
% File that maps UNIX-style inodes to data block locations in Datanodes
% Write-Ahead log in stable storage

All metadata is kept in memory on the NameNode. As explained above, the metadata holds a UNIX-style inode and location on DataNodes for each file. These locations are represented as block locations in virtual memory of each of the DataNodes. Meaning that upon creation of a file, the NameNode will allocate a block for the file in virtual memory on however many DataNodes the file will be replicated across (the default is 3 replicas). Then, the NameNode is responsible for pointing the clients to the appropriate blocks to write or read files, and also maintaining the namespace. The NameNode keeps the maps to data blocks up-to-date by communicating with the DataNodes via ``heartbeats" (i.e., TCP packets). The DataNodes are responsible for sending periodic heartbeats that contain block reports to indicate that they are functioning correctly and can be reached along the network. The block report gives the block locations of all files in the virtual memory of that DataNode. The NameNode uses this information to ensure each block is replicated enough times and keep the locations of the blocks up to date. 

Clearly durability of the NameNode is very important to the system. There are various schemes to ensure a durable NameNode. A write ahead log is maintained on stable storage and this can be used to build a BackupNode or CheckpointNode that essentially maintains a copy of the NameNode. 


%\subsection{DataNode}

% Stores file's data blocks
% Periodic heartbeat to notify Namenode of its health (i.e. if it's running)
% Periodic block reports to notify Namenode of a stored block's status (e.g. block write recieved)

%TODO cut this section/ merge with above?



\subsection{Reads and Writes in HDFS}
% Perhaps add section for how Read / Write operations are processed in HDFS? Might help describe the type of operations Namenode has to process.
% E.g. Client Request Workflow for reading data

% -------------------------------------------------

Files in HDFS are append-only and can only be written to by a single writer at a time so that once data is written it cannot be overwritten. This simplifies concurrency control and allows the system to guarantee that writes can be read as soon as a file is closed. On a write, a client must first query the NameNode to find which DataNodes and blocks to write. Then, the client will ensure a replicated write by sending data in TCP-style packets through a pipeline including all DataNodes over which the file will be replicated. This guarantees that the writes will be consistent across multiple DataNodes. On a read, a client must first query the NameNode to find which DataNodes contain replicas of the desired file and which blocks hold the file. Then, the client reads from the closest available DataNodes directly. 


\section{Limitations of Single Namenode Architecture}

Despite the benefits of the single NameNode architecture listed above, this architectural decision causes some potentially major scalability problems. These problems will be especially pronounced when HDFS is used to store many small files as opposed to few large files (although many large files will also prove difficult HDFS). 

\subsection{Physical Memory Size}

% Re-iterate in-memory metadata namespace
% Total File Capacity of Cluster is Limited by Name Node's RAM due to requirement that all metadata objects (file inodes and block) in memory at all times

% SVENCHO PAPER EXAMPLE: Assuming on average, a file consists of \lambda = 1.5 blocks, then each file uses 1 file and ~2 data block objects, hence uses ~600 bytes. Thus to keep 100 million files, name-node must have at least 60GM of RAM


The obvious problem with forcing all metadata to be in memory on the NameNode is that the NameNode will run out of available memory. %todo cite Shvanko

Let $ M $ be the size of memory on the NameNode in bytes, $ \lambda $ be the number of blocks allocated to each file, and $ F $ be the number of files on the system. Also, let $ c $ be the number of bytes to represent a single block mapping to all replicas of a block, and $ i $ the number of bytes to represent and inode. If we want to fit all metadata into memory, it is clear that we must have:
\begin{align*}
	M > \lambda c F + i F
\end{align*}
	This limit is much easier to reach with small files since each file is allocated its own virtual block, no matter how small the file is, i.e. we have that $ \lambda \geq 1$. This means that creating many small files should take up $ c + i$ bytes for each additional file, while growing a file by a similar amount will use at most $ c $ bytes. Thus, the memory limit is much more of a problem for many small files. 


\subsection{Namenode CPU Bottleneck}

The less obvious scalability problem with the HDFS architecture is the potential for a CPU bottleneck at the NameNode. The system is built on the assumption that the NameNode operations are small enough and there will be few enough large files that they will be insignificant. However, with many small files, these assumptions are violated and the NameNode CPU could become a bottleneck. 

% Describe the INTERNAL-LOAD communicating with Datanodes
% # of Heartbeats that Namenode recieves is directly proportional to # of Datanodes in cluster
% # of Block reports is also directly proportional to size of data blocks (i.e. ratio of blocks mapped per file) and also # of replicated copies per block
% Physical data storage and data I/O performance increase proportionally to # of Datanodes in cluster and size of data blocks, but the overall performance of HDFS is also affected negatively by increased internal load.
% The more time spend on internal load, less time spend on processing external client's requests

The first potential source of CPU bottleneck is the internal-load of communication with the DataNodes. This internal-load can be created in two ways. First, internal-load can be created when the system holds a large amount of small files. The size of the block report sent by each DataNode is directly proportional to the number of blocks that the DataNode holds. Since each file is given its own virtual block, even if the file is much smaller than a block, the DataNode can potentially hold a great number of small files and have to send a large block report containing the metadata of each of these files. This could potentially overload the NameNode CPU. 
Second, the number of block reports that a NameNode receives is directly proportional to the number of DataNodes in the system. So, if the system has a large number of DataNodes, it is possible to generate a large amount of internal load by having many DataNodes.  


% Describe the external-load from clients
% All metadata operations run on Namenode - they cannot be processed anywhere else.
% On every file read, there is a get_block_location per block per file
% On every file write, there is a create_block per block per file
% Namenode can become a bottleneck with operations that involve many metadata operations, e.g. batch-write operations will include many create_block requests

% -------------------------------------------------

Another potential source of CPU bottleneck is external-load from clients querying the NameNode. As explained above, on both reads and writes a client must first contact the NameNode to find the appropriate block locations and then the DataNodes for the read or write. Afterwards the NameNode must perform a metadata operation to determine what to return to the client. Therefore, the NameNode can become a bottleneck if there are many clients trying to perform operations concurrently since they all must access the metadata via the singular NameNode. This should be an especially prevalent problem when there are many small files because it would be likely that the use pattern of a system with many small files would be for there to be many clients each trying to access their files concurrently. 

With these potential CPU bottlenecks in mind, we pose the question of whether it is possible to generate an appropriate load so as to reach these CPU bottlenecks before the NameNode runs out of memory. This will be addressed by our experimentation. 

\section{Performance Evaluation}

% Discuss goals of tests (i.e. "we want to test Memory and CPU and determine blah")

% Mention experimental setup (EC2, Java Test Skeleton, # nodes, etc)


% FOR EACH EXPERIMENT:
% Discuss designs of individual experiments and their results
% What were the experiments testing?
% What were the different parameters of the experiments?
% Possibilities of error?

\subsection{Experiment 1: Memory Experiments}

% TODO: See above

\subsection{Experiment 2: CPU Experiments}

% TODO: See above

\subsection{Analysis}

% Discuss raw results independently for each experiment:
% Are these results what was expected (and why)?
% If there was some error, discuss what may have caused it
% Graphs

% Evaluate experimental results holistically to pinpoint the problem with HDFS's Single Namenode architecture (I.e. weigh CPU and Memory problems against each other)

% -------------------------------------------------

\section{Improvements to Single Namenode Architecture}

% Use experimental data to determine what is actually detrimental to performance of HDFS, and suggest a way to improve it

% -------------------------------------------------

% Would it be helpful to remove the assumption that each file is assigned to its own block? Or alternatively, somehow batch block-reports for small files

% mention HDFS federation, CalvinFS
% 

\section{Conclusion}

% Wrap it up with stuff.



%TODO sources

\end{multicols*}
\end{document}


























 ``